{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPccqe8zacHPiWeVh0oDQbe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karalius/Calculator/blob/master/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp72u7kPmXOi",
        "outputId": "830b6c33-de75-4152-feee-2ce2fdc875fb"
      },
      "source": [
        "!pip install fake-useragent"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.7/dist-packages (0.1.11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDK9JKiXTaQ",
        "outputId": "cab96d07-eaf2-4bdd-e1fc-73b63270883c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from fake_useragent import UserAgent\n",
        "from lxml.html import fromstring\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Set\n",
        "import psycopg2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import re"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGKqxzKac6f"
      },
      "source": [
        "def get_proxies() -> Set:\n",
        "    url = 'https://www.us-proxy.org/'\n",
        "    response = requests.get(url)\n",
        "    parser = fromstring(response.text)\n",
        "    proxies = set()\n",
        "\n",
        "    # Extract proxy string and add to the set\n",
        "    for i in parser.xpath('//tbody/tr')[:20]:\n",
        "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
        "            proxies.add(proxy)\n",
        "            \n",
        "    return proxies"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu_u094bopE9"
      },
      "source": [
        "def replace_attr(html_doc: str, from_attr: str, to_attr: str) -> str:\n",
        "    soup = BeautifulSoup(html_doc.content, 'html.parser')\n",
        "    tags = soup(attrs={from_attr: True})\n",
        "\n",
        "    # Replace tags with new tag\n",
        "    for tag in tags:\n",
        "        tag[to_attr] = tag[from_attr]\n",
        "        del tag[from_attr]\n",
        "\n",
        "    return soup"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCiRA686dJyo"
      },
      "source": [
        "# Input options: ['prints', 'painting', 'photography'], 3000\n",
        "\n",
        "def scrape_etsy(keywords: list, items_to_scrape: int) -> pd.DataFrame:\n",
        "    average_items_per_page = 45\n",
        "    pages_to_scrape = math.ceil(items_to_scrape / average_items_per_page)\n",
        "    df_list = [] \n",
        "    ua = UserAgent(use_cache_server=False)\n",
        "\n",
        "    for key in keywords:\n",
        "        titles, prices, item_urls, img_urls = ([] for i in range(4))\n",
        "        adj_keyword = key.lower()\n",
        "        proxies = get_proxies()\n",
        "\n",
        "        for page_no in range(1, pages_to_scrape + 1):\n",
        "            for proxy in proxies:\n",
        "                # Test if proxy works and get page html\n",
        "                try:\n",
        "                    headers = {\n",
        "                        'authority': 'www.etsy.com',\n",
        "                        'sec-ch-ua': '^\\\\^Google',\n",
        "                        'sec-ch-ua-mobile': '?0',\n",
        "                        'upgrade-insecure-requests': '1',\n",
        "                        'user-agent': str(ua.random),\n",
        "                        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "                        'sec-fetch-site': 'same-origin',\n",
        "                        'sec-fetch-mode': 'navigate',\n",
        "                        'sec-fetch-user': '?1',\n",
        "                        'sec-fetch-dest': 'document',\n",
        "                        'referer': f'https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no-1}',\n",
        "                        'accept-language': 'en-US,en;q=0.9',\n",
        "                    }\n",
        "                    params = (\n",
        "                        ('explicit', '1'),\n",
        "                        ('ref', 'pagination'),\n",
        "                        ('page', f'{page_no}'),\n",
        "                    )\n",
        "                    page = requests.get(\n",
        "                        f\"https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no}\",\n",
        "                        headers = headers,\n",
        "                        params = params,\n",
        "                        proxies={\"http\": str(proxy), \"https\": str(proxy)}\n",
        "                    )\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    continue\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "            # Replace data-src to src attributes to stay consistent\n",
        "            soup = replace_attr(page,'data-src', 'src')\n",
        "            \n",
        "            for value in soup.find_all('div', class_ = ['js-merch-stash-check-listing']):\n",
        "                # Discard trees with discount price\n",
        "                if value.find(class_='strike-through'):\n",
        "                    value.unwrap()\n",
        "\n",
        "                # Extract and append to lists information on each item\n",
        "                titles.extend([title.get('title') for title in value.find_all('a')])\n",
        "                prices.extend([float(price.get_text().replace(',','')) for price in value.find_all('span', class_='currency-value')])\n",
        "                item_urls.extend([link.get('href') for link in value.find_all('a')])\n",
        "                img_urls.extend([pic.img['src'] for pic in value.find_all('div', class_='height-placeholder')])\n",
        "\n",
        "            #  Sleep anywhere from 0.4s to 1.2s\n",
        "            time.sleep(np.random.uniform(0.4, 1.2))\n",
        "        \n",
        "        # Append dataframes of each keyword and limit length to items_to_scrape\n",
        "        df_list.append(pd.DataFrame({\n",
        "        'category': adj_keyword[:items_to_scrape],\n",
        "        'title': titles[:items_to_scrape],\n",
        "        'price': prices[:items_to_scrape],\n",
        "        'item_url': item_urls[:items_to_scrape],\n",
        "        'img_url': img_urls[:items_to_scrape]\n",
        "    }))\n",
        "   \n",
        "    return pd.concat(df_list, ignore_index=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSABn_10_9h0"
      },
      "source": [
        "# Call scrape_etsy function and name it df\n",
        "df = scrape_etsy(['painting', 'photography', 'prints'], 80)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gZz_2ovqxrI",
        "outputId": "256eb718-52ca-4838-d15b-8988bc1a7c4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 240 entries, 0 to 239\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   category  240 non-null    object \n",
            " 1   title     240 non-null    object \n",
            " 2   price     240 non-null    float64\n",
            " 3   item_url  240 non-null    object \n",
            " 4   img_url   240 non-null    object \n",
            "dtypes: float64(1), object(4)\n",
            "memory usage: 9.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wom0G51aeEhv"
      },
      "source": [
        "def create_sql_tables() -> None:\n",
        "\n",
        "    # Connect to Herokuw Postgres\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=\"dfvnovppbbq4rl\",\n",
        "        user=\"yntjjrygcmheyc\",\n",
        "        password=\"477cb32bc389caa14bd09da3c5c4a5ee704213bce0033af8db470ea5fbd2f5de\",\n",
        "        host=\"ec2-34-254-69-72.eu-west-1.compute.amazonaws.com\",\n",
        "        port=\"5432\"\n",
        "    )\n",
        "    \n",
        "    cur = sql_connection.cursor()\n",
        "    \n",
        "    # Create two tables in DB\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS categories (\n",
        "        id serial PRIMARY KEY,\n",
        "        category varchar(250)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE IF NOT EXISTS items (\n",
        "        id serial PRIMARY KEY,\n",
        "        category_id int,\n",
        "        title varchar(250),\n",
        "        price float(2),\n",
        "        item_url varchar(500),\n",
        "        img_url varchar(500),\n",
        "        FOREIGN KEY (category_id) REFERENCES categories(id)\n",
        "    );\n",
        "\n",
        "    ''')\n",
        "    \n",
        "    # Insert category names to the categories table\n",
        "    for i in df['category'].unique():\n",
        "        cur.execute(f\"INSERT INTO categories (category) VALUES ('{i}');\")\n",
        "\n",
        "    sql_connection.commit()\n",
        "\n",
        "    sql_connection.close()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRehRweeq53G"
      },
      "source": [
        "create_sql_tables()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0a4u-iFekN4"
      },
      "source": [
        "def items_dataframe() -> pd.DataFrame:\n",
        "\n",
        "    # Create category id for each category\n",
        "    foreign_key_df = pd.DataFrame(df['category'].unique(), columns=['category']).reset_index().rename(columns={'index': 'category_id'})\n",
        "    foreign_key_df['category_id'] = np.arange(1, len(foreign_key_df)+1)\n",
        "\n",
        "    # Make df only with category id\n",
        "    items_df = pd.merge(\n",
        "        df,\n",
        "        foreign_key_df,\n",
        "        on='category'\n",
        "    ).drop(columns='category')\n",
        "\n",
        "    # Put category id as first column\n",
        "    items_df.insert(0, 'category_id', items_df.pop('category_id'))\n",
        "\n",
        "    return items_df"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdnKLQjaesNF"
      },
      "source": [
        "def items_to_heroku() -> None:\n",
        "    from sqlalchemy import create_engine\n",
        "\n",
        "    # Connect to Heroku Postgres\n",
        "    conn = create_engine('postgresql://yntjjrygcmheyc:477cb32bc389caa14bd09da3c5c4a5ee704213bce0033af8db470ea5fbd2f5de@ec2-34-254-69-72.eu-west-1.compute.amazonaws.com:5432/dfvnovppbbq4rl')\n",
        "\n",
        "    # Get items_df\n",
        "    items_df = items_dataframe()\n",
        "\n",
        "    # Push items df to sql\n",
        "    items_df.to_sql('items', conn, method='multi', if_exists='append', chunksize=10000, index=False)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfFSpMQZq9DH"
      },
      "source": [
        "items_to_heroku()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RevC0MQWex9Q"
      },
      "source": [
        "def get_csv() -> None:\n",
        "    from google.colab import files\n",
        "    # Connect to Heroku Postgres\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=\"dfvnovppbbq4rl\",\n",
        "        user=\"yntjjrygcmheyc\",\n",
        "        password=\"477cb32bc389caa14bd09da3c5c4a5ee704213bce0033af8db470ea5fbd2f5de\",\n",
        "        host=\"ec2-34-254-69-72.eu-west-1.compute.amazonaws.com\",\n",
        "        port=\"5432\"\n",
        "    )\n",
        "\n",
        "    cur = sql_connection.cursor()\n",
        "\n",
        "    # Join tables on category id\n",
        "    s = \"SELECT items.id, categories.category, items.title, items.price, items.item_url, items.img_url FROM items JOIN categories ON categories.id = items.category_id ORDER BY id ASC\"\n",
        "\n",
        "    # COPY function on the SQL we created above.\n",
        "    SQL_for_file_output = \"COPY ({0}) TO STDOUT WITH CSV HEADER\".format(s)\n",
        "\n",
        "    # Set up a variable to store our file path and name.\n",
        "\n",
        "    #t_path_n_file = \"C:\\Users\\37069\\Desktop\"\n",
        "\n",
        "    with open('etsy_data.csv', 'w') as f_output:\n",
        "        cur.copy_expert(SQL_for_file_output, f_output)\n",
        "\n",
        "    files.download('etsy_data.csv')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EGi9OCjMezi1",
        "outputId": "e3f3d1a7-26b3-466d-ae62-a2e89a2326d3"
      },
      "source": [
        "get_csv()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a7a35148-483f-434c-b9db-1c0217b7d651\", \"etsy_data.csv\", 101268)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}