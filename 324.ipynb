{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "324.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karalius/Calculator/blob/master/324.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vpH6h58Y6ae"
      },
      "source": [
        "# Module 3: Machine Learning\n",
        "\n",
        "## Sprint 2: Intermediate Machine Learning\n",
        "\n",
        "## Subproject 4: Gradient boosted trees models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACDbDIFnY6aj"
      },
      "source": [
        "Previously we learned about random forest models, where the main idea is to fit many decision tree models, where each decision tree is built by using a random subset of the training samples and/or the features. While a single decision tree is likely to be a weak classifier that does not generalize well, the collective decision of all of them usually improves the generalization capability. One important aspect of random forest learning is that each tree learns independently - all of them try to be accurate on the whole training set, in other words, all of them try to minimize the same objective loss.\n",
        "\n",
        "It is quite popular to use a different training strategy - have each tree learn using a different objective, one that essentially says \"give more weight to the samples that are misclassified by the trees trained before\". A model that uses such a strategy, gradient boosted trees, will be the topic of this notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HV3RIDdY6ak"
      },
      "source": [
        "## Learning outcomes\n",
        "\n",
        "- Gradient boosting\n",
        "- Gradient boosted tree models in scikit-learn and xgboost packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb-pWcbGY6ak"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUbOPBY7Y6al"
      },
      "source": [
        "## Gradient boosted trees introduction\n",
        "\n",
        "Imagine if there was a team-based competition that would test each team on their knowledge of several subjects - math, literature and computer science. One potential strategy would be for each team member to learn the subjects independently, maybe randomly picking the books from which to learn from, so that two different members would learn math from different books and their learning would be different. Given a competition question, each of the members would vote on the answer and the team's answer would be the most common answer among the team members. This is similar to random forest learning, where each member is a decision tree. This kind of independent parallel learning is also called bagging.\n",
        "\n",
        "An intuitive way to improve upon that strategy would be for members to focus on the subject in which the other team members are currently weaker at, this might improve the overall team's performance. This is the essence of the boosting idea - each model puts more value on learning the samples where the models before it fails at. This kind of dependent sequantial learning is called boosting.\n",
        "\n",
        "To continue learning about boosting algorithms, read the article below:\n",
        "\n",
        "- https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning\n",
        "\n",
        "Again look up the documentation for this type of model in scikit-learn:\n",
        "\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
        "\n",
        "However, even though scikit-learn has this type of model, a very popular package for gradient boosting is called xgboost. Start by reading about it here:\n",
        "\n",
        "- https://xgboost.readthedocs.io/en/latest/tutorials/model.html\n",
        "\n",
        "The next lesson in the \"Intermediate Machine Learning\" course is about it too, so it will help to read and do the exercise:\n",
        "\n",
        "- https://www.kaggle.com/alexisbcook/xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnloMSvSY6am"
      },
      "source": [
        "## Gradient boosted trees in practice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBjDA7xMZnK4",
        "outputId": "c1fce80b-17c9-4555-c35c-bab9db34377c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install scikit-learn --upgrade"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 80.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p98WqoLpY6am"
      },
      "source": [
        "As usual, begin by importing required modules and setting the random state:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqfinCAuY6an"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "RANDOM_STATE = 7"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Id1ebpfY6ap"
      },
      "source": [
        "We will use California housing dataset again. This will also allow us to compare results with the previously trained models, as we are using the same random state value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml2iqhqSY6aq",
        "outputId": "7f148e8c-579a-4708-d089-42ddb7b21bd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "x, y = datasets.fetch_california_housing(\n",
        "    return_X_y=True,\n",
        "    as_frame=True\n",
        ")\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_split(\n",
        "    x,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "x.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MedInc</th>\n",
              "      <th>HouseAge</th>\n",
              "      <th>AveRooms</th>\n",
              "      <th>AveBedrms</th>\n",
              "      <th>Population</th>\n",
              "      <th>AveOccup</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.3252</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6.984127</td>\n",
              "      <td>1.023810</td>\n",
              "      <td>322.0</td>\n",
              "      <td>2.555556</td>\n",
              "      <td>37.88</td>\n",
              "      <td>-122.23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.3014</td>\n",
              "      <td>21.0</td>\n",
              "      <td>6.238137</td>\n",
              "      <td>0.971880</td>\n",
              "      <td>2401.0</td>\n",
              "      <td>2.109842</td>\n",
              "      <td>37.86</td>\n",
              "      <td>-122.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.2574</td>\n",
              "      <td>52.0</td>\n",
              "      <td>8.288136</td>\n",
              "      <td>1.073446</td>\n",
              "      <td>496.0</td>\n",
              "      <td>2.802260</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5.6431</td>\n",
              "      <td>52.0</td>\n",
              "      <td>5.817352</td>\n",
              "      <td>1.073059</td>\n",
              "      <td>558.0</td>\n",
              "      <td>2.547945</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.8462</td>\n",
              "      <td>52.0</td>\n",
              "      <td>6.281853</td>\n",
              "      <td>1.081081</td>\n",
              "      <td>565.0</td>\n",
              "      <td>2.181467</td>\n",
              "      <td>37.85</td>\n",
              "      <td>-122.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   MedInc  HouseAge  AveRooms  ...  AveOccup  Latitude  Longitude\n",
              "0  8.3252      41.0  6.984127  ...  2.555556     37.88    -122.23\n",
              "1  8.3014      21.0  6.238137  ...  2.109842     37.86    -122.22\n",
              "2  7.2574      52.0  8.288136  ...  2.802260     37.85    -122.24\n",
              "3  5.6431      52.0  5.817352  ...  2.547945     37.85    -122.25\n",
              "4  3.8462      52.0  6.281853  ...  2.181467     37.85    -122.25\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh9uP2LRY6ar"
      },
      "source": [
        "Let's build a baseline gradient boosted trees regressor model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yu6GROxHY6ar",
        "outputId": "e00e9b50-be05-4b3e-b843-41e2c64195f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "gbr = GradientBoostingRegressor(\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=100,\n",
        "    subsample=1.0,\n",
        "    max_depth=3,\n",
        "    max_features=1.0,\n",
        "    verbose=1,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "gbr.fit(x_train, y_train)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss   Remaining Time \n",
            "         1           1.1920            4.23s\n",
            "         2           1.0770            4.13s\n",
            "         3           0.9843            4.04s\n",
            "         4           0.9079            3.99s\n",
            "         5           0.8402            4.04s\n",
            "         6           0.7856            3.96s\n",
            "         7           0.7401            3.90s\n",
            "         8           0.6973            3.90s\n",
            "         9           0.6634            3.88s\n",
            "        10           0.6349            3.85s\n",
            "        20           0.4717            3.37s\n",
            "        30           0.3812            2.98s\n",
            "        40           0.3337            2.54s\n",
            "        50           0.3079            2.11s\n",
            "        60           0.2925            1.67s\n",
            "        70           0.2823            1.25s\n",
            "        80           0.2751            0.83s\n",
            "        90           0.2678            0.41s\n",
            "       100           0.2617            0.00s\n",
            "CPU times: user 4.14 s, sys: 16.2 ms, total: 4.16 s\n",
            "Wall time: 4.15 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fQN9LgKY6ar"
      },
      "source": [
        "If any of the parameters used above are unclear, you can consult the documentation - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html.\n",
        "\n",
        "Based on the output above, we can see how the training loss is reduced after each of the training iterations. Remaining time is also shown for our convenience. The training time is fast too, at least compared to the heavier neural network models we trained earlier.\n",
        "\n",
        "We can check the model's performance:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px3Pab7rY6as",
        "outputId": "a50f44d2-9248-4b49-e042-267996cf2c21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "y_pred_train = gbr.predict(x_train)\n",
        "print('Training mean squared error is', mean_squared_error(y_train, y_pred_train))\n",
        "\n",
        "y_pred = gbr.predict(x_val)\n",
        "print('Validation mean squared error is', mean_squared_error(y_val, y_pred))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean squared error is 0.2617311992553173\n",
            "Validation mean squared error is 0.30095959244265574\n",
            "CPU times: user 46.1 ms, sys: 1.93 ms, total: 48 ms\n",
            "Wall time: 47.4 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7BOnTZ2Y6at"
      },
      "source": [
        "The performance seems good, at least compared to the models we trained previously (the neural network with hyper-parameters found via optimization got around 0.31 validation error).\n",
        "\n",
        "Let's build a model with a larger number of deeper trees. Additionally, let's only use a random subsample of our training data. Having held-out data will also help the training procedure to compute the so called OOB Improve score, which basically means the improvement on the validation samples. You can read more about it here: https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_oob.html (optional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNdt6T0xY6at",
        "outputId": "bf767643-c061-4669-a3e1-91b82d5429b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "gbr = GradientBoostingRegressor(\n",
        "    learning_rate=0.1,\n",
        "    n_estimators=300,\n",
        "    subsample=0.8,\n",
        "    max_depth=5,\n",
        "    max_features=1.0,\n",
        "    verbose=1,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "gbr.fit(x_train, y_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Iter       Train Loss      OOB Improve   Remaining Time \n",
            "         1           1.1642           0.1580           17.79s\n",
            "         2           1.0266           0.1355           17.19s\n",
            "         3           0.9223           0.1103           17.64s\n",
            "         4           0.8308           0.0929           17.28s\n",
            "         5           0.7576           0.0770           16.92s\n",
            "         6           0.6847           0.0652           16.74s\n",
            "         7           0.6294           0.0530           16.75s\n",
            "         8           0.5774           0.0449           16.60s\n",
            "         9           0.5409           0.0424           16.65s\n",
            "        10           0.5112           0.0337           16.59s\n",
            "        20           0.3427           0.0096           15.71s\n",
            "        30           0.2686           0.0020           14.96s\n",
            "        40           0.2343           0.0022           14.29s\n",
            "        50           0.2186           0.0003           13.69s\n",
            "        60           0.2013           0.0000           13.14s\n",
            "        70           0.1942          -0.0002           12.58s\n",
            "        80           0.1822           0.0003           12.02s\n",
            "        90           0.1762           0.0002           11.47s\n",
            "       100           0.1711          -0.0003           10.91s\n",
            "       200           0.1297          -0.0002            5.45s\n",
            "       300           0.1028          -0.0001            0.00s\n",
            "CPU times: user 16.3 s, sys: 34.3 ms, total: 16.3 s\n",
            "Wall time: 16.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gigYj_s7Y6at"
      },
      "source": [
        "The training was a bit longer now, but still fast. Let's see how it performs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyYtQdXgY6at",
        "outputId": "55d31c90-a220-4d61-e7d8-5329b32ef324",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "y_pred_train = gbr.predict(x_train)\n",
        "print('Training mean squared error is', mean_squared_error(y_train, y_pred_train))\n",
        "\n",
        "y_pred = gbr.predict(x_val)\n",
        "print('Validation mean squared error is', mean_squared_error(y_val, y_pred))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training mean squared error is 0.10213083736924554\n",
            "Validation mean squared error is 0.2240998504181583\n",
            "CPU times: user 159 ms, sys: 4.93 ms, total: 164 ms\n",
            "Wall time: 164 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEQ_q7B5Y6au"
      },
      "source": [
        "We might say that the model overfitted the training data a bit, as the validation error is much higher than the training error, however, the validation error still looks good.\n",
        "\n",
        "We can also look at the feature importances of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilp3w4hOY6au",
        "outputId": "27cecb85-b265-4a8c-9421-6d63d1c79876",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "feature_importance_df = pd.DataFrame(\n",
        "    {\n",
        "        'feature': x_train.columns,\n",
        "        'importance': gbr.feature_importances_\n",
        "    }\n",
        ")\n",
        "feature_importance_df"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>importance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MedInc</td>\n",
              "      <td>0.534399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HouseAge</td>\n",
              "      <td>0.042794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AveRooms</td>\n",
              "      <td>0.035974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AveBedrms</td>\n",
              "      <td>0.017305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Population</td>\n",
              "      <td>0.016042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>AveOccup</td>\n",
              "      <td>0.132981</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Latitude</td>\n",
              "      <td>0.108937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Longitude</td>\n",
              "      <td>0.111569</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      feature  importance\n",
              "0      MedInc    0.534399\n",
              "1    HouseAge    0.042794\n",
              "2    AveRooms    0.035974\n",
              "3   AveBedrms    0.017305\n",
              "4  Population    0.016042\n",
              "5    AveOccup    0.132981\n",
              "6    Latitude    0.108937\n",
              "7   Longitude    0.111569"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUexAKLyY6au"
      },
      "source": [
        "We can see that the important features are similar to the important features found by using linear regression that we used in a previous notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x06pGNqtY6av"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "Go through the notebook for the titanic competition: https://www.kaggle.com/datacanary/xgboost-example-python\n",
        "\n",
        "Do this task:\n",
        "- Improve the public score by changing something in the training procedure\n",
        "\n",
        "It might be useful to consult xgboost documentation (https://xgboost.readthedocs.io/en/latest/parameter.html) to see if there are parameters you could try to optimize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1T8ecmCY6av"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGQnHBHBY6av"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this notebook we learned about a popular type of a machine learning model - gradient boosted trees. This model quite commonly wins Kaggle competitions when the input is of tabular data type (not images or free text), and while the underlying training process can seem complex, it is enough to understand the main idea to be able to use it well - while training a collection of learners, give more weight to those training samples that were misclassified by previous learners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fr7po8IY6av"
      },
      "source": [
        "## Further research"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLRrwvkPY6av"
      },
      "source": [
        "- https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting\n",
        "- https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
        "- https://arxiv.org/pdf/1603.02754.pdf"
      ]
    }
  ]
}