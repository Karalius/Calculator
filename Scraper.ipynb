{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZe5uYJRZgOsBCbZSpTHEQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karalius/Calculator/blob/master/Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp72u7kPmXOi",
        "outputId": "59b089e9-673c-433d-c3a3-7e9e2667abb7"
      },
      "source": [
        "!pip install fake-useragent\n",
        "!pip install pyopenssl"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.7/dist-packages (0.1.11)\n",
            "Requirement already satisfied: pyopenssl in /usr/local/lib/python3.7/dist-packages (20.0.1)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyopenssl) (1.15.0)\n",
            "Requirement already satisfied: cryptography>=3.2 in /usr/local/lib/python3.7/dist-packages (from pyopenssl) (3.4.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=3.2->pyopenssl) (1.14.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=3.2->pyopenssl) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxDK9JKiXTaQ"
      },
      "source": [
        "from fake_useragent import UserAgent\n",
        "from sqlalchemy import create_engine\n",
        "from lxml.html import fromstring\n",
        "from google.colab import files\n",
        "from bs4 import BeautifulSoup\n",
        "from typing import Set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import psycopg2\n",
        "import requests\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import re"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGKqxzKac6f"
      },
      "source": [
        "def get_proxies() -> Set:\n",
        "    url = 'https://www.us-proxy.org/'\n",
        "    response = requests.get(url)\n",
        "    parser = fromstring(response.text)\n",
        "    proxies = set()\n",
        "\n",
        "    # Extract proxy string and add to the set\n",
        "    for i in parser.xpath('//tbody/tr')[:60]:\n",
        "        if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
        "            proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
        "            proxies.add(proxy)\n",
        "\n",
        "    # Up to 10 US proxies in the set\n",
        "    return proxies"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vu_u094bopE9"
      },
      "source": [
        "def replace_attr(html_doc: str, from_attr: str, to_attr: str) -> str:\n",
        "    soup = BeautifulSoup(html_doc.content, 'html.parser')\n",
        "    tags = soup(attrs={from_attr: True})\n",
        "\n",
        "    # Replace tags with new tag\n",
        "    for tag in tags:\n",
        "        tag[to_attr] = tag[from_attr]\n",
        "        del tag[from_attr]\n",
        "\n",
        "    return soup"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCiRA686dJyo"
      },
      "source": [
        "# Available keywords: 'painting', 'photography', 'prints'\n",
        "# Input: up to 3 keywords and items to scrape\n",
        "\n",
        "def scrape_etsy(keywords: list, items_to_scrape: int) -> pd.DataFrame:\n",
        "    average_items_per_page = 64\n",
        "    pages_to_scrape = math.ceil(items_to_scrape / average_items_per_page)\n",
        "    df_list = []\n",
        "\n",
        "    # Define fake user agent\n",
        "    ua = UserAgent(use_cache_server=False)\n",
        "\n",
        "    for key in keywords:\n",
        "        titles, prices, item_urls, img_urls = ([] for i in range(4))\n",
        "        adj_keyword = key.lower()\n",
        "        # Get new set of proxies for each keyword\n",
        "        proxies_set = get_proxies()\n",
        "        \n",
        "        for page_no in range(1, pages_to_scrape + 1):\n",
        "            proxies_to_iter = proxies_set\n",
        "\n",
        "            for proxy in proxies_to_iter.copy():\n",
        "                # Test if proxy works and get page html\n",
        "                try:\n",
        "                    headers = {\n",
        "                        'authority': 'www.etsy.com',\n",
        "                        'sec-ch-ua': '^\\\\^Google',\n",
        "                        'sec-ch-ua-mobile': '?0',\n",
        "                        'upgrade-insecure-requests': '1',\n",
        "                        'user-agent': str(ua.random),\n",
        "                        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',\n",
        "                        'sec-fetch-site': 'same-origin',\n",
        "                        'sec-fetch-mode': 'navigate',\n",
        "                        'sec-fetch-user': '?1',\n",
        "                        'sec-fetch-dest': 'document',\n",
        "                        'referer': f'https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no-1}',\n",
        "                        'accept-language': 'en-US,en;q=0.9',\n",
        "                    }\n",
        "                    params = (\n",
        "                        ('explicit', '1'),\n",
        "                        ('ref', 'pagination'),\n",
        "                        ('page', f'{page_no}'),\n",
        "                    )\n",
        "                    page = requests.get(\n",
        "                        f\"https://www.etsy.com/c/art-and-collectibles/{adj_keyword}?explicit=1&ship_to=US&ref=pagination&page={page_no}\",\n",
        "                        headers = headers,\n",
        "                        params = params,\n",
        "                        proxies={\"http\": str(proxy), \"https\": str(proxy)}\n",
        "                    )\n",
        "                except requests.exceptions.RequestException as e:\n",
        "                    # Remove timed out proxy from the set\n",
        "                    proxies_to_iter.discard(proxy)\n",
        "                    continue\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "            # Replace data-src to src attributes to stay consistent\n",
        "            soup = replace_attr(page,'data-src', 'src')\n",
        "            \n",
        "            for value in soup.find_all('div', class_ = ['js-merch-stash-check-listing']):\n",
        "                # Discard trees with discount price\n",
        "                if value.find(class_='strike-through'):\n",
        "                    value.unwrap()\n",
        "\n",
        "                # Extract and append to lists information on each item\n",
        "                titles.extend([title.get('title') for title in value.find_all('a')])\n",
        "                prices.extend([float(price.get_text().replace(',','')) for price in value.find_all('span', class_='currency-value')])\n",
        "                item_urls.extend([link.get('href') for link in value.find_all('a')])\n",
        "                img_urls.extend([pic.img['src'] for pic in value.find_all('div', class_='height-placeholder')])\n",
        "\n",
        "            #  Sleep anywhere from 0.4s to 1.2s\n",
        "            time.sleep(np.random.uniform(0.4, 1.2))\n",
        "        \n",
        "        # Append dataframes of each keyword and limit length to items_to_scrape\n",
        "        df_list.append(pd.DataFrame({\n",
        "        'category': adj_keyword[:items_to_scrape],\n",
        "        'title': titles[:items_to_scrape],\n",
        "        'price': prices[:items_to_scrape],\n",
        "        'item_url': item_urls[:items_to_scrape],\n",
        "        'img_url': img_urls[:items_to_scrape]\n",
        "    }))\n",
        "   \n",
        "    return pd.concat(df_list, ignore_index=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSABn_10_9h0"
      },
      "source": [
        "# Call scrape_etsy function and name it df\n",
        "df = scrape_etsy(['painting', 'photography', 'prints'], 3000)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT3LK-7Gz5hX"
      },
      "source": [
        "# Heroku DB credentials, define here:\n",
        "database = \"dfvnovppbbq4rl\"\n",
        "user = \"yntjjrygcmheyc\"\n",
        "password = \"477cb32bc389caa14bd09da3c5c4a5ee704213bce0033af8db470ea5fbd2f5de\"\n",
        "host = \"ec2-34-254-69-72.eu-west-1.compute.amazonaws.com\"\n",
        "port = \"5432\"\n",
        "for_engine = 'postgresql://yntjjrygcmheyc:477cb32bc389caa14bd09da3c5c4a5ee704213bce0033af8db470ea5fbd2f5de@ec2-34-254-69-72.eu-west-1.compute.amazonaws.com:5432/dfvnovppbbq4rl'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wom0G51aeEhv"
      },
      "source": [
        "def create_sql_tables() -> None:\n",
        "    # Heroku credentials\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=database,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        host=host,\n",
        "        port=port\n",
        "    )\n",
        "    # Connect to DB\n",
        "    cur = sql_connection.cursor()\n",
        "    \n",
        "    # Create two tables\n",
        "    cur.execute('''\n",
        "    CREATE TABLE IF NOT EXISTS categories (\n",
        "        id serial PRIMARY KEY,\n",
        "        category varchar(250)\n",
        "    );\n",
        "\n",
        "    CREATE TABLE IF NOT EXISTS items (\n",
        "        id serial PRIMARY KEY,\n",
        "        category_id int,\n",
        "        title varchar(250),\n",
        "        price float(2),\n",
        "        item_url varchar(500),\n",
        "        img_url varchar(500),\n",
        "        FOREIGN KEY (category_id) REFERENCES categories(id)\n",
        "    );\n",
        "\n",
        "    ''')\n",
        "    \n",
        "    # Insert category names to the categories table\n",
        "    for i in df['category'].unique():\n",
        "        cur.execute(f\"INSERT INTO categories (category) VALUES ('{i}');\")\n",
        "\n",
        "    sql_connection.commit()\n",
        "\n",
        "    sql_connection.close()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRehRweeq53G"
      },
      "source": [
        "create_sql_tables()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdnKLQjaesNF"
      },
      "source": [
        "def items_to_heroku() -> None:\n",
        "    # Create category id for each category\n",
        "    foreign_key_df = pd.DataFrame(df['category'].unique(), columns=['category']).reset_index().rename(columns={'index': 'category_id'})\n",
        "    foreign_key_df['category_id'] = np.arange(1, len(foreign_key_df)+1)\n",
        "\n",
        "    # Make df only with category id\n",
        "    items_df = pd.merge(\n",
        "        df,\n",
        "        foreign_key_df,\n",
        "        on='category'\n",
        "    ).drop(columns='category')\n",
        "\n",
        "    # Put category id as first column\n",
        "    items_df.insert(0, 'category_id', items_df.pop('category_id'))\n",
        "\n",
        "    # Connect to Heroku Postgres\n",
        "    conn = create_engine(for_engine)\n",
        "\n",
        "    # Push items df to Heroku DB\n",
        "    items_df.to_sql('items', conn, method='multi', if_exists='append', chunksize=10000, index=False)\n",
        "\n",
        "    conn.dispose()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfFSpMQZq9DH"
      },
      "source": [
        "items_to_heroku()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RevC0MQWex9Q"
      },
      "source": [
        "def get_csv() -> None:\n",
        "    # Heroku credentials\n",
        "    sql_connection = psycopg2.connect(\n",
        "        database=database,\n",
        "        user=user,\n",
        "        password=password,\n",
        "        host=host,\n",
        "        port=port\n",
        "    )\n",
        "    # Connect to DB\n",
        "    cur = sql_connection.cursor()\n",
        "\n",
        "    # Join tables on category id\n",
        "    s = \"SELECT items.id, categories.category, items.title, items.price, items.item_url, items.img_url FROM items JOIN categories ON categories.id = items.category_id ORDER BY id ASC\"\n",
        "\n",
        "    # COPY function on the SQL we created above.\n",
        "    SQL_for_file_output = \"COPY ({0}) TO STDOUT WITH CSV HEADER\".format(s)\n",
        "\n",
        "    # Set up a variable to store our file path and name.\n",
        "\n",
        "    #t_path_n_file = \"C:\\Users\\37069\\Desktop\"\n",
        "\n",
        "    with open('etsy_data.csv', 'w') as f_output:\n",
        "        cur.copy_expert(SQL_for_file_output, f_output)\n",
        "\n",
        "    files.download('etsy_data.csv')\n",
        "\n",
        "    sql_connection.close()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "EGi9OCjMezi1",
        "outputId": "b16acf11-1614-4ebb-f323-3fbdbdf68a38"
      },
      "source": [
        "get_csv()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_850eb79f-3384-4083-9c32-d1269a2b544c\", \"etsy_data.csv\", 25268)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}